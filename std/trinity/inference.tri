module std.trinity.inference

// Provable private neural inference with hash commitment and quantum circuit.
//
// Four domains, one program:
//   Phase 1 — Privacy: LWE homomorphic encryption (real TFHE over Goldilocks)
//   Phase 2 — Neural:  Dense layer (matvec + bias + lookup-table ReLU)
//   Phase 3 — Crypto:  Poseidon2 hash commitment (binds model parameters)
//   Phase 4 — Quantum: 2-qubit Bell pair commitment circuit
//
// The ReLU activation is a RAM-based lookup table — the same table that
// would serve as the FHE programmable bootstrapping test polynomial.
// One table, multiple readers: the Rosetta Stone stepping stone.
//
// Poseidon2 round constants and the ReLU table both live in RAM,
// authenticated by the STARK consistency argument.
//
// Pitch parameters:
//   LWE dimension 8, 8 encrypted inputs, 16-neuron hidden layer,
//   Poseidon2 hash commitment, 2-qubit Bell commitment.
use vm.core.field

use vm.core.convert

use vm.core.assert

use vm.io.mem

use std.nn.tensor

use std.fhe.lwe

use std.math.lut

use std.crypto.poseidon2

use std.quantum.gates

// ---------------------------------------------------------------------------
// Phase 1: Private linear layer via LWE encryption
// ---------------------------------------------------------------------------
// Real homomorphic computation: encrypted input vector multiplied by
// plaintext weight matrix. Each output is an LWE ciphertext encoding
// the weighted sum. The STARK proof covers every field operation.
pub fn private_linear(
    cts_addr: Field,
    w_addr: Field,
    ct_out_addr: Field,
    tmp_addr: Field,
    lwe_n: Field,
    input_dim: Field,
    neurons: Field
) {
    lwe.private_linear(cts_addr, w_addr, ct_out_addr, tmp_addr, lwe_n, input_dim, neurons)
}

// ---------------------------------------------------------------------------
// Phase 1b: Decrypt outputs
// ---------------------------------------------------------------------------
// Bridge between encrypted Phase 1 and plaintext Phase 2.
// Each output ciphertext is decrypted via io.divine() — the prover
// supplies the plaintext, the circuit verifies the noise bound.
pub fn decrypt_outputs(
    ct_out_addr: Field,
    s_addr: Field,
    result_addr: Field,
    delta: Field,
    lwe_n: Field,
    neurons: Field
) {
    let stride: Field = lwe_n + 1
    for i in 0..neurons bounded 4096 {
        let idx: Field = convert.as_field(i)
        let ct_addr: Field = ct_out_addr + idx * stride
        let m: Field = lwe.decrypt(ct_addr, s_addr, delta, lwe_n)
        mem.write(result_addr + idx, m)
    }
}

// ---------------------------------------------------------------------------
// Phase 2: Neural dense layer with lookup-table activation
// ---------------------------------------------------------------------------
// Dense layer: out = lut_relu(W * x + b).
// Matrix-vector multiply, bias addition, then ReLU via lookup table.
//
// The lookup table is the Rosetta Stone primitive:
//   - Here it serves as the neural network activation function.
//   - The same table IS the FHE programmable bootstrapping test polynomial.
//   - The STARK proof authenticates all table reads through RAM consistency.
//
// lut_addr: precomputed ReLU table in RAM (built by lut.build_relu)
pub fn dense_layer(
    w_addr: Field,
    x_addr: Field,
    b_addr: Field,
    out_addr: Field,
    tmp_addr: Field,
    lut_addr: Field,
    neurons: Field
) {
    tensor.matvec(w_addr, x_addr, tmp_addr, neurons, neurons)
    tensor.bias_add(tmp_addr, b_addr, out_addr, neurons)
    lut.apply(lut_addr, out_addr, out_addr, neurons)
}

// ---------------------------------------------------------------------------
// Phase 3: Hash commitment (Poseidon2)
// ---------------------------------------------------------------------------
// Binds the proof to specific model parameters and inference output.
// Hashes (weights_digest, key_digest, output_digest, class) into a
// single field element. The verifier checks this digest matches the
// expected value — proving "THIS model with THIS key produced THIS
// result", not just "some model produced some result".
//
// Round constants are read from RAM at rc_addr (86 field elements).
// output_digest is computed as the sum of activated outputs (a simple
// commitment; a Merkle root would be stronger for large vectors).
pub fn hash_commit(
    activated_addr: Field,
    neurons: Field,
    weights_digest: Field,
    key_digest: Field,
    class: Field,
    rc_addr: Field
) -> Field {
    // Compute output digest: sum of activated values
    let mut output_digest: Field = 0
    for i in 0..neurons bounded 4096 {
        let idx: Field = convert.as_field(i)
        output_digest = output_digest + mem.read(activated_addr + idx)
    }
    // Hash (weights_digest, key_digest, output_digest, class) -> digest
    poseidon2.hash4_to_digest(weights_digest, key_digest, output_digest, class, rc_addr)
}

// ---------------------------------------------------------------------------
// Phase 4: Quantum commitment (2-qubit Bell pair)
// ---------------------------------------------------------------------------
// Superdense coding commitment circuit:
//   |00> -> H(q0) -> CNOT -> conditional CZ -> CNOT -> H(q0) -> measure q0
//
// class=0: |00> -> Bell -> skip CZ -> decode -> |00> -> p0>p1 -> true
// class>0: |00> -> Bell -> CZ -> decode -> |10> -> p0<p1 -> false
//
// Measurement traces out q1 from the 2-qubit state (sum of two norm-squareds
// per outcome), so we can't use gates.measure_deterministic directly
// (which handles single-qubit |alpha|^2 vs |beta|^2). The comparison
// logic is identical — split + threshold over Goldilocks.
pub fn quantum_commit(class: Field) -> Bool {
    let q0: gates.Qubit = gates.init_zero()
    let q1: gates.Qubit = gates.init_zero()
    let q0h: gates.Qubit = gates.hadamard(q0)
    let bell: gates.TwoQubit = gates.two_qubit_product(q0h, q1)
    let entangled: gates.TwoQubit = gates.cnot(bell)
    let mut committed: gates.TwoQubit = entangled
    if class {
        committed = gates.cz(entangled)
    }
    let decoded: gates.TwoQubit = gates.cnot(committed)
    // H on q0: apply to 2-qubit state
    let q00: gates.Complex = gates.complex_add(decoded.q00, decoded.q10)
    let q01: gates.Complex = gates.complex_add(decoded.q01, decoded.q11)
    let q10: gates.Complex = gates.complex_sub(decoded.q00, decoded.q10)
    let q11: gates.Complex = gates.complex_sub(decoded.q01, decoded.q11)
    // Trace out q1: p0 = |q00|^2 + |q01|^2, p1 = |q10|^2 + |q11|^2
    let p0: Field = gates.complex_norm_sq(q00) + gates.complex_norm_sq(q01)
    let p1: Field = gates.complex_norm_sq(q10) + gates.complex_norm_sq(q11)
    let diff: Field = p0 + field.neg(p1)
    let (hi, lo) = convert.split(diff)
    let threshold: U32 = convert.as_u32(2147483647)
    hi < threshold
}

// ---------------------------------------------------------------------------
// Full Trinity pipeline
// ---------------------------------------------------------------------------
// 1. Private linear layer (LWE homomorphic encryption — FHE)
// 2. Decrypt encrypted outputs (bridge to plaintext)
// 3. Dense neural layer with lookup-table ReLU (AI + Rosetta Stone)
// 4. Poseidon2 hash commitment (binds model + key + output — Crypto)
// 5. Quantum commitment (2-qubit Bell circuit — Quantum)
//
// The lookup table at lut_addr serves dual purpose:
//   - Neural activation (ReLU in Phase 2)
//   - FHE test polynomial (same table would drive PBS)
// One table, multiple readers. The STARK proof covers all reads.
//
// Poseidon2 round constants at rc_addr serve the hash commitment.
// Both the LUT and round constants live in RAM, authenticated by
// the STARK consistency argument.
//
// The class is computed inside the pipeline via argmax on the dense layer
// output. expected_class is the prover's claim — the circuit asserts it
// matches the computed argmax, preventing class substitution without
// performing the actual inference.
//
// The hash digest binds this specific model (weights_digest), this
// specific key (key_digest), and the computed output to the proof.
// expected_digest is the prover's claim — the circuit asserts it matches.
//
// Returns: Bool from quantum measurement confirming the commitment.
pub fn trinity(
    cts_addr: Field,
    s_addr: Field,
    w_priv_addr: Field,
    ct_out_addr: Field,
    tmp_addr: Field,
    result_addr: Field,
    delta: Field,
    lwe_n: Field,
    input_dim: Field,
    neurons: Field,
    dense_w_addr: Field,
    dense_b_addr: Field,
    activated_addr: Field,
    lut_addr: Field,
    expected_class: Field,
    rc_addr: Field,
    weights_digest: Field,
    key_digest: Field,
    expected_digest: Field
) -> Bool {
    // Phase 1: Encrypted linear layer
    private_linear(cts_addr, w_priv_addr, ct_out_addr, tmp_addr, lwe_n, input_dim, neurons)
    // Phase 1b: Decrypt
    decrypt_outputs(ct_out_addr, s_addr, result_addr, delta, lwe_n, neurons)
    // Phase 2: Dense neural layer (activation via shared lookup table)
    dense_layer(dense_w_addr, result_addr, dense_b_addr, activated_addr, tmp_addr, lut_addr, neurons)
    // Compute class from neural output — data dependency between AI and Quantum
    let class: Field = tensor.argmax(activated_addr, neurons)
    assert.eq(class, expected_class)
    // Phase 3: Hash commitment — binds model parameters to proof
    let digest: Field = hash_commit(activated_addr, neurons, weights_digest, key_digest, class, rc_addr)
    assert.eq(digest, expected_digest)
    // Phase 4: Quantum commitment on computed class
    quantum_commit(class)
}
