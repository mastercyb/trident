// Hand-optimized TASM baseline: std.trinity.inference
//
// Provable private neural inference with quantum commitment.
// Three-domain integration: Privacy (FHE polynomial ops),
// Neural (ReLU activation), Quantum (Deutsch oracle).
//
// Stack convention:
//   Arguments pushed left-to-right (first arg deepest on stack).
//   Return values left on top of stack after return.
//   RAM-based functions operate on addresses (field element pointers).
//
// This module's functions are compositions of poly.*, tensor.*, and gates.*
// primitives. The baseline measures the overhead of arg setup and call
// dispatch â€” the inner functions have their own baselines.
//
// Instruction count rules:
//   - Comments (// ...) are NOT counted
//   - Labels (ending with :) are NOT counted
//   - halt is NOT counted
//   - Blank lines are NOT counted
//   - Everything else IS counted (including return)
//
// Static instruction count summary:
//   __private_neuron  :  12
//   __private_linear  :  45
//   __activate        :  11
//   __quantum_commit  :   3
//   __trinity         :  21
//   ----------------------------------------
//   Total             :  92


// ===========================================================================
// PHASE 1: PRIVATE LINEAR LAYER (FHE polynomial arithmetic)
// ===========================================================================


// ---------------------------------------------------------------------------
// __private_neuron: (input_addr, weight_addr, out_addr, x, n) -> Field
// ---------------------------------------------------------------------------
// Pointwise multiply input with weight polynomial, then evaluate at x.
// FHE-style: encrypted dot product via polynomial arithmetic.
//
// Stack on entry: [n, x, out_addr, weight_addr, input_addr]
//
// Step 1: pointwise_mul(input_addr, weight_addr, out_addr, n)
// Step 2: eval(out_addr, x, n)
// Step 3: cleanup non-return args
//
// 12 counted instructions.
__private_neuron:
    // --- pointwise_mul(input_addr, weight_addr, out_addr, n) ---
    dup 4
    dup 4
    dup 4
    dup 4
    call __pointwise_mul
    // Stack unchanged: [n, x, out_addr, weight_addr, input_addr]
    // --- eval(out_addr, x, n) ---
    dup 2
    dup 2
    dup 2
    call __eval
    // Stack: [result, n, x, out_addr, weight_addr, input_addr]
    swap 5
    pop 5
    return


// ---------------------------------------------------------------------------
// __private_linear: (input_addr, w0, w1, w2, w3, result_addr, tmp_addr, x, n)
// ---------------------------------------------------------------------------
// Full private linear layer: 4 neurons via polynomial products.
//
// Stack on entry: [n, x, tmp_addr, result_addr, w3, w2, w1, w0, input_addr]
//
// For each neuron i: private_neuron(input_addr, w_i, tmp_addr, x, n)
// then write result to result_addr + i.
//
// 45 counted instructions.
__private_linear:
    // --- neuron 0: private_neuron(input, w0, tmp, x, n) ---
    dup 8
    dup 8
    dup 4
    dup 4
    dup 4
    call __private_neuron
    dup 4
    write_mem 1
    pop 1
    // --- neuron 1 ---
    dup 8
    dup 7
    dup 4
    dup 4
    dup 4
    call __private_neuron
    dup 4
    push 1
    add
    write_mem 1
    pop 1
    // --- neuron 2 ---
    dup 8
    dup 6
    dup 4
    dup 4
    dup 4
    call __private_neuron
    dup 4
    push 2
    add
    write_mem 1
    pop 1
    // --- neuron 3 ---
    dup 8
    dup 5
    dup 4
    dup 4
    dup 4
    call __private_neuron
    dup 4
    push 3
    add
    write_mem 1
    pop 1
    // Cleanup
    pop 5
    pop 4
    return


// ===========================================================================
// PHASE 2: NEURAL ACTIVATION
// ===========================================================================


// ---------------------------------------------------------------------------
// __activate: (result_addr, bias_addr, out_addr)
// ---------------------------------------------------------------------------
// Bias add + ReLU on 4-element vector.
//
// Stack on entry: [out_addr, bias_addr, result_addr]
//
// Step 1: bias_add(result_addr, bias_addr, out_addr, 4)
// Step 2: relu_layer(out_addr, out_addr, 4)
//
// 11 counted instructions.
__activate:
    dup 2
    dup 2
    dup 2
    push 4
    call __bias_add
    dup 0
    dup 1
    push 4
    call __relu_layer
    pop 3
    return


// ===========================================================================
// PHASE 3: QUANTUM COMMITMENT
// ===========================================================================


// ---------------------------------------------------------------------------
// __quantum_commit: (class: Field) -> Bool
// ---------------------------------------------------------------------------
// Deutsch-style oracle: |0> -> H -> (Z if class) -> H -> measure
//
// Mathematical reduction (all intermediate steps verified):
//   init_zero:  zero=(1,0), one=(0,0)
//   hadamard:   zero=(1,0), one=(1,0)
//   pauliz:     one=(-1,0) when class!=0, one=(1,0) when class==0
//   hadamard:   zero=(1+one_re, 0), one=(1-one_re, 0)
//   measure:    |zero|^2 vs |one|^2
//
//   class==0: zero=(2,0), one=(0,0) -> |zero|^2=4 > |one|^2=0 -> true
//   class!=0: zero=(0,0), one=(2,0) -> |zero|^2=0 < |one|^2=4 -> false
//
// The full quantum circuit reduces to: class == 0.
// This is the optimal instruction sequence for this Deutsch oracle.
//
// 3 counted instructions.
__quantum_commit:
    push 0
    eq
    return


// ===========================================================================
// FULL TRINITY PIPELINE
// ===========================================================================


// ---------------------------------------------------------------------------
// __trinity: (input_addr, w0, w1, w2, w3, bias_addr, result_addr,
//             activated_addr, tmp_addr, x, n, expected_class) -> Bool
// ---------------------------------------------------------------------------
// Full pipeline: private_linear -> activate -> quantum_commit.
//
// Stack on entry:
//   [expected_class, n, x, tmp_addr, activated_addr, result_addr,
//    bias_addr, w3, w2, w1, w0, input_addr]
//
// 21 counted instructions.
__trinity:
    // --- private_linear(input, w0, w1, w2, w3, result, tmp, x, n) ---
    dup 11
    dup 11
    dup 11
    dup 11
    dup 11
    dup 7
    dup 7
    dup 7
    dup 7
    call __private_linear
    // --- activate(result_addr, bias_addr, activated_addr) ---
    dup 5
    dup 7
    dup 6
    call __activate
    // --- quantum_commit(expected_class) ---
    dup 0
    call __quantum_commit
    // Stack: [result, expected_class, n, x, ...]
    // Cleanup: remove all 12 original args, keep result
    swap 12
    pop 5
    pop 5
    pop 2
    return
